{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random                                                              \n",
    "                                                                           \n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cleaned=pd.read_csv(r'data/processed/ratings_cleaned.csv')\n",
    "books_cleaned = pd.read_csv(r'data/processed/books_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users: 16027 | Total Books: 9999\n",
      "Sparsity: 99.5%\n"
     ]
    }
   ],
   "source": [
    "# print total number of users and total books we are looking at\n",
    "num_users = ratings_cleaned.user_id.unique().shape[0]\n",
    "num_books = ratings_cleaned.book_id.unique().shape[0]\n",
    "print(\"Total Users: \" + str(\n",
    "    num_users) + \" | Total Books: \" + str(num_books))\n",
    "\n",
    "# sparsity of data\n",
    "sparsity = round(1.0-len(ratings_cleaned)/float(num_users*num_books),3)\n",
    "print(\"Sparsity: \" + str(sparsity*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.19 GiB for an array with shape (16027, 9999) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c30d0c14cc72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# convert ratings to pivot table and fills nan with 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpivot_ratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mratings_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'book_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpivot_ratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpivot_ratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.apply(lambda row: row.fillna(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# check which books were not rated and add them in as a column of 0s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   5921\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5923\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5925\u001b[0m     _shared_docs[\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mindexed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(self, level, fill_value)\u001b[0m\n\u001b[0;32m   3548\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3550\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3552\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mconstructor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_expanddim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         )\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0munstacker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_new_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_new_values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m             \u001b[0mnew_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.19 GiB for an array with shape (16027, 9999) and data type float64"
     ]
    }
   ],
   "source": [
    "# convert ratings to pivot table and fills nan with 0\n",
    "pivot_ratings = ratings_cleaned.pivot(index='user_id', columns='book_id', values='rating')\n",
    "pivot_ratings = pivot_ratings.fillna(0)#.apply(lambda row: row.fillna(0)\n",
    "\n",
    "# check which books were not rated and add them in as a column of 0s\n",
    "# book_id_not = []\n",
    "# book_id_possible = np.arange(1,10001)\n",
    "\n",
    "# for i in book_id_possible:\n",
    "#     if i not in ratings_cleaned['book_id'].unique():\n",
    "#         book_id_not.append(i)\n",
    "\n",
    "# display(book_id_not)\n",
    "#master_table.insert(loc=8616, column=8616, value=0)\n",
    "pivot_ratings.insert(loc=8882, column=8882, value=0)\n",
    "\n",
    "pivot_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_ratings_knn = csr_matrix(pivot_ratings.T.values)\n",
    "knn = NearestNeighbors(metric='cosine',algorithm='brute')\n",
    "knn.fit(pivot_ratings_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert book name to book_id\n",
    "book_id = books_cleaned[books_cleaned['title']=='Twilight (Twilight, #1)']['book_id'].values[0]\n",
    "# find the 10 most similar books based on cosine similarity\n",
    "book_distances, book_indices = knn.kneighbors(pivot_ratings.T.loc[book_id,:].values.reshape(1,-1),n_neighbors=10)\n",
    "\n",
    "display(book_distances)\n",
    "book_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(book_distances.flatten())):\n",
    "        if i ==0:\n",
    "            print('Books similar to book {0}:\\n'.format('Twilight (Twilight, #1)')) #item_master_table.index[user_id]\n",
    "        else:\n",
    "            print('{0}: {1}, with distance of {2}'.format(i,books_cleaned[books_cleaned['book_id']==(book_indices.flatten()[i]+1)]['title'].values[0], book_distances.flatten()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_item_cf(book_title, num_books):\n",
    "    # gets top 9 similar books and similarity score\n",
    "    # converts book title to book_id\n",
    "    book_id = books_cleaned[books_cleaned['title']==book_title]['book_id'].values\n",
    "    book_distances,book_indices = knn.kneighbors(pivot_ratings.T.loc[book_id,:].values.reshape(1,-1),n_neighbors=num_books+1)\n",
    "    for i in range(0,len(book_distances.flatten())):\n",
    "        if i ==0:\n",
    "            print('')\n",
    "            print('Books similar to book {0}:\\n'.format(book_title))\n",
    "        else:\n",
    "            print('{0}: {1}, with distance of {2}'.format(i,books_cleaned[books_cleaned['book_id']==(book_indices.flatten()[i]+1)]['title'].values[0],book_distances.flatten()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_cf('Twilight (Twilight, #1)', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_cf(user_id, threshold_rating, num_books):\n",
    "    find_books_similar_to = ratings_cleaned[(ratings_cleaned['user_id'] == user_id) & (ratings_cleaned['rating'] >= threshold_rating)]['title'].array\n",
    "    for book in find_books_similar_to:\n",
    "        item_item_cf(book, num_books)\n",
    "    print('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_cf(7,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_cleaned[ratings_cleaned['user_id']==19]\n",
    "pivot_ratings_knn = csr_matrix(pivot_ratings.values)\n",
    "knn = NearestNeighbors(metric='cosine',algorithm='brute')\n",
    "knn.fit(pivot_ratings_knn)\n",
    "\n",
    "user_distances,user_indices = knn.kneighbors(pivot_ratings.loc[19,:].values.reshape(1,-1),n_neighbors=30)\n",
    "similar_users = pivot_ratings.index[user_indices.flatten()]\n",
    "\n",
    "for i in range(0,10):\n",
    "    if i ==0:\n",
    "            # if user is user given\n",
    "        print('Users similar to user {0}:\\n'.format('19')) #master_table.index[user_id]\n",
    "    else:\n",
    "            # else print top 9 users \n",
    "            # gets location top users and prints their user_id\n",
    "        print('{0}: {1}, with distance of {2}'.format(i,pivot_ratings.index[user_indices.flatten()[i]],user_distances.flatten()[i]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_cf_df = ratings_cleaned.groupby('user_id').agg(np.mean).drop(['book_id'],axis=1)\n",
    "\n",
    "user_based_cf_df = ratings_cleaned.merge(user_based_cf_df, how='inner',on='user_id')\n",
    "user_based_cf_df['mean_difference'] = user_based_cf_df['rating_x'] - user_based_cf_df['rating_y']\n",
    "user_based_cf_df\n",
    "\n",
    "# rating_x : rating given by user_id to book_id\n",
    "# rating_y : average rating of user_id\n",
    "# mean_difference: rating_x - rating_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_mean_difference = pd.pivot_table(user_based_cf_df,values='mean_difference',index='user_id',columns='book_id')\n",
    "pivot_mean_difference = pivot_mean_difference.fillna(0) #user_master_table.mean(axis=0))\n",
    "pivot_mean_difference.insert(loc=8882, column=8882, value=0)\n",
    "pivot_mean_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = sum(1 - user_distances.flatten()) # denominator\n",
    "similarity_scores_T = 1-user_distances.T # similarity scores for each user\n",
    "# putting the equation together\n",
    "pivot_mean_difference = pivot_mean_difference.loc[similar_users,:].multiply(similarity_scores_T/similarity_scores, axis=0)\n",
    "# entire fraction summation\n",
    "pivot_mean_difference.loc['Column_Total'] = pivot_mean_difference.sum(axis=0) \n",
    "\n",
    "# user 19's average rating\n",
    "u_avg_rating = ratings_cleaned.groupby('user_id').agg(np.mean).loc[19,'rating']# 3.2083333333333335\n",
    "\n",
    "# adding user 19's average rating to each user-book pair score and sorting them from highest to lowest\n",
    "recs = sorted(list(enumerate(pivot_mean_difference.loc['Column_Total'] + u_avg_rating, 1)),key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list into dataframe only showing top 10 books\n",
    "predicted_books = pd.DataFrame(recs,columns=['book_id','predicted_rating'])[:10]\n",
    "display(predicted_books.head())\n",
    "\n",
    "\n",
    "# dataframe showing how similar users rated the books recommended\n",
    "display(user_based_cf_df[(user_based_cf_df['user_id'].isin(similar_users)) & (user_based_cf_df['book_id'].isin(predicted_books['book_id']))].sort_values('book_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_books['book_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_based_cf(user_id,k_neighbors, n_books):\n",
    "    '''\n",
    "        Finds the closest neighbors (k_neighbors) in terms of cosine similarity for a given user (user_id) and\n",
    "        Returns the top books (n_books) the model predicts the user will rate highly \n",
    "        \n",
    "    '''\n",
    "\n",
    "    user_distances,user_indices = knn.kneighbors(pivot_ratings.loc[user_id,:].values.reshape(1,-1),n_neighbors=k_neighbors)\n",
    "    similar_users = pivot_ratings.index[user_indices.flatten()]\n",
    "\n",
    "    for i in range(0,len(user_distances.flatten())):\n",
    "        if i ==0:\n",
    "                # if user is user given\n",
    "            print('Users similar to user {0}:\\n'.format(user_id)) #master_table.index[user_id]\n",
    "        else:\n",
    "                # else print top 9 users \n",
    "                # gets location top users and prints their user_id\n",
    "            print('{0}: {1}, with distance of {2}'.format(i,pivot_ratings.index[user_indices.flatten()[i]],user_distances.flatten()[i]))\n",
    "\n",
    "    user_based_cf_df = ratings_cleaned.groupby('user_id').agg(np.mean).drop(['book_id'],axis=1)\n",
    "    user_based_cf_df = user_based_cf_df.merge(ratings_cleaned, how='inner',on='user_id')\n",
    "    user_based_cf_df['mean_difference'] = user_based_cf_df['rating_x'] - user_based_cf_df['rating_y']\n",
    "\n",
    "    similarity_scores = sum(1 - user_distances.flatten()) # denominator\n",
    "    similarity_scores_T = 1-user_distances.T # similarity scores for each user\n",
    "    # putting the equation together\n",
    "    pivot_mean_difference_func = pivot_mean_difference.loc[similar_users,:].multiply(similarity_scores_T/similarity_scores, axis=0)\n",
    "    # entire fraction summation\n",
    "    pivot_mean_difference_func.loc['Column_Total'] = pivot_mean_difference_func.sum(axis=0) \n",
    "\n",
    "    # user's average rating\n",
    "    u_avg_rating = ratings_cleaned.groupby('user_id').agg(np.mean).loc[user_id,'rating']# 3.2083333333333335\n",
    "\n",
    "    # adding user's average rating to each user-book pair score and sorting them from highest to lowest\n",
    "    recs = sorted(list(enumerate(pivot_mean_difference_func.loc['Column_Total'] + u_avg_rating, 1)),key=lambda x: x[1],reverse=True)\n",
    "    # recs in a dataframe\n",
    "    recs_df = pd.DataFrame(recs,columns=['book_id','predicted_rating'])[:n_books]\n",
    "    \n",
    "    for i in range(0,len(recs_df)):\n",
    "        if i ==0:\n",
    "                # if user is user given\n",
    "            print('')\n",
    "            print('Top {0} Books liked by {0} people most similar to user 19:\\n'.format(n_books, k_neighbors)) #master_table.index[user_id]\n",
    "        else:\n",
    "                # else print top 9 users \n",
    "                # gets location top users and prints their user_id\n",
    "            print('{0}: {1}, with your predicted rating of {2}'.format(i,recs_df['book_id'][i],recs_df['predicted_rating'][i]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_based_cf(19,30,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% trainset, 20% testset  \n",
    "shuffled = pivot_ratings.replace(0,np.NaN).sample(frac = 1,axis=0,random_state=42)\n",
    "threshold = int(.8 * len(shuffled))                                     \n",
    "train = shuffled[:threshold]                             \n",
    "test = shuffled[threshold:]\n",
    "train_indices = train.index.values\n",
    "test_indices = test.index.values\n",
    "\n",
    "# normalize ratings\n",
    "train_means = np.nan_to_num(np.mean(train,axis=1).values)\n",
    "train_demeaned = train.sub(train_means,axis=0).replace(np.NaN,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10 SVD components from train matrix\n",
    "start10 = time.time()\n",
    "u,s,vt = svds(train_demeaned,k=10)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "svd_10_rmse = mean_squared_error(X_pred,train_demeaned,squared=True)\n",
    "end10 = time.time()\n",
    "print('SVD Using 10 Features RMSE: ' + str(svd_10_rmse))\n",
    "print(end10 - start10)\n",
    "\n",
    "# get 20 SVD components from train matrix\n",
    "start20 = time.time()\n",
    "u,s,vt = svds(train_demeaned,k=20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "svd_20_rmse = mean_squared_error(X_pred,train_demeaned,squared=True)\n",
    "end20 = time.time()\n",
    "print('SVD Using 20 Features RMSE: ' + str(svd_20_rmse))\n",
    "print(end20 - start20)\n",
    "\n",
    "# get 50 SVD components from train matrix\n",
    "start50 = time.time()\n",
    "u,s,vt = svds(train_demeaned,k=50)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "svd_50_rmse = mean_squared_error(X_pred,train_demeaned,squared=True)\n",
    "end50 = time.time()\n",
    "print('SVD Using 50 Features RMSE: ' + str(svd_50_rmse))\n",
    "print(end50 - start50)\n",
    "\n",
    "# get 100 SVD components from train matrix\n",
    "start100 = time.time()\n",
    "u,s,vt = svds(train_demeaned,k=100)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "svd_100_rmse = mean_squared_error(X_pred,train_demeaned,squared=True)\n",
    "end100 = time.time()\n",
    "print('SVD Using 100 Features RMSE: ' + str(svd_100_rmse))\n",
    "print(end100 - start100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user-book pivot table with weighted ratings as values\n",
    "pivot_weighted_ratings = books_weighted_ranking.merge(ratings_cleaned, how='inner',on='book_id').pivot(index='user_id', columns='book_id', values='weighted_score').fillna(0)\n",
    "# insert book that wasn't rated\n",
    "pivot_weighted_ratings.insert(loc=8882, column=8882, value=0)\n",
    "\n",
    "# split data\n",
    "# weighted trainset is the same as svd trainset\n",
    "weightedshuffled = pivot_weighted_ratings.loc[train_indices,:]\n",
    "start_weighted = time.time()\n",
    "weightedpopular_rmse = mean_squared_error(weightedshuffled,train_demeaned,squared=True)\n",
    "end_weighted = time.time()\n",
    "print(end_weighted - start_weighted)\n",
    "print('Weight-Based RMSE: ' + str(weightedpopular_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = [svd_10_rmse,svd_20_rmse,svd_50_rmse,svd_100_rmse, weightedpopular_rmse]\n",
    "models = ['Train SVD with 10 Features','Train SVD 20 Features','Train SVD 50 Features','Train SVD 100 Features', 'Train Weighted Average']\n",
    "plt.bar(models, rmse);\n",
    "plt.xticks(rotation=90);\n",
    "plt.ylabel('RMSE');\n",
    "plt.title('RMSE for Different Collaborative Filtering Models');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize ratings\n",
    "test_nan = np.nan_to_num(np.mean(test,axis=1).values)\n",
    "test_demeaned = test.sub(test_nan,axis=0).replace(np.NaN,0)\n",
    "\n",
    "\n",
    "# get 10 SVD components from train matrix\n",
    "starttest = time.time()\n",
    "u,s,vt = svds(test_demeaned,k=10)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "test_svd_10_rmse = mean_squared_error(X_pred,test_demeaned,squared=True)\n",
    "endtest = time.time()\n",
    "print('Test RMSE Using 10 Features: ' + str(test_svd_10_rmse))\n",
    "print(endtest - starttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = [test_svd_10_rmse,svd_10_rmse,svd_20_rmse,svd_50_rmse,svd_100_rmse, weightedpopular_rmse]\n",
    "models = ['Test SVD with 10 Features','Train SVD with 10 Features','Train SVD 20 Features','Train SVD 50 Features','Train SVD 100 Features', 'Train Weighted Average']\n",
    "plt.bar(models, rmse);\n",
    "plt.xticks(rotation=90);\n",
    "plt.ylabel('RMSE');\n",
    "plt.title('RMSE for Different Collaborative Filtering Models');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_ratings_nan = pivot_ratings.replace(0,np.NaN)\n",
    "\n",
    "\n",
    "# get means for each user\n",
    "user_means = np.nan_to_num(np.mean(pivot_ratings_nan,axis=1).values)\n",
    "# subtract means from dataset and replace not-yet-rated items with 0\n",
    "all_demeaned = pivot_ratings_nan.sub(user_means,axis=0).replace(np.NaN,0)\n",
    "\n",
    "# get 10 SVD components from train matrix\n",
    "start10 = time.time()\n",
    "u,s,vt = svds(all_demeaned,k=10)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred_all = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "all_svd_10_rmse = mean_squared_error(X_pred_all,all_demeaned,squared=True)\n",
    "end10 = time.time()\n",
    "all_svd_10_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features by scaling each feature from 1 to 5\n",
    "sc=MinMaxScaler(feature_range = (1,5))\n",
    "svd_mat = sc.fit_transform(X_pred_all.T + user_means)\n",
    "\n",
    "# convert scaled matrix to df\n",
    "svd_df = pd.DataFrame(svd_mat.T, columns=pivot_ratings_nan.columns,index=pivot_ratings_nan.index)\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_rec(user_id,num_recs):\n",
    "    print('Books user {0} has highly rated read:'.format(user_id))\n",
    "    print(user_based_cf_df[(user_based_cf_df['user_id'] == user_id) & (user_based_cf_df['rating_x'] > 3)][['title', 'rating_x','rating_y']])\n",
    "   \n",
    "    # dataframe of all books and predicted scores, sorted\n",
    "    user_recs = pd.DataFrame(svd_df.loc[user_id].sort_values(ascending=False).reset_index()).rename(columns = {user_id : 'predicted_rating'})\n",
    "    # find books that user has already read\n",
    "    user_read = user_based_cf_df[user_based_cf_df['user_id'] == user_id]\n",
    "    # take away books the user has read\n",
    "    # df: book_id | predicted_rating | title\n",
    "    new_books = user_recs[~user_recs['book_id'].isin(user_read)][:num_recs].merge(ratings_cleaned[['book_id','title']],on='book_id',how='inner').drop_duplicates().reset_index()\n",
    "    new_books\n",
    "    \n",
    "    # recommend the top n books with their titles and predicted rating\n",
    "    print('')\n",
    "    print('Top {0} Books we think user {1} would rate the highest:\\n'.format(num_recs, user_id))\n",
    "\n",
    "    for i in range(0,len(new_books)):\n",
    "        print('{0}: {1}, with your predicted rating of {2}'.format(i+1,new_books['title'][i],new_books['predicted_rating'][i]))\n",
    "\n",
    "\n",
    "def predict_rating(user_id,book_title):\n",
    "    # we will use SVD with 10 features\n",
    "    book_id = books_cleaned[books_cleaned['title']==book_title]['book_id'].values\n",
    "    print('The predicted rating for user {0} for {1} is {2}'.format(user_id,book_title,svd_df.loc[user_id,book_id])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_rec(10560,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rating(52414,'Harry Potter and the Half-Blood Prince (Harry Potter, #6)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
