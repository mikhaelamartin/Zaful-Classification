{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "- Numerical vs. Target (Correlation)\n",
    "- Numerical vs. Numerical (Correlation)\n",
    "- Categorical vs. Target (Chi-Squared)\n",
    "- Categorical vs. Categorical (Chi-Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('')\n",
    "y_train = \n",
    "X_test = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender (1), one hot encode, drop_first\n",
    "# card_category: blue or not (1), one hot encode\n",
    "# marital_status (2), one hot encode, drop_first\n",
    "# income_category (1) label\n",
    "# education_level (1) label \n",
    "\n",
    "def categorical_to_numerical(df):\n",
    "    \n",
    "    # ordinal: one hot encode\n",
    "    if 'Marital_Status' in df.columns:\n",
    "        df_dummies = pd.get_dummies(df[['Gender','Marital_Status']],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop(['Gender','Marital_Status'],axis=1,inplace=True)\n",
    "    else:\n",
    "        df_dummies = pd.get_dummies(df['Gender'],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop('Gender',axis=1,inplace=True)\n",
    "    \n",
    "    # nominal: label encode\n",
    "    if 'Card_Category' in df.columns:\n",
    "        card_mapping = {'Blue' : 0, 'Silver' : 1, 'Gold' : 2, 'Platinum' : 3}\n",
    "        df['Card_Category'] = df['Card_Category'].map(card_mapping)\n",
    "    \n",
    "    edu_mapping = {'Uneducated' : 0, 'High School' : 1, 'College' : 2, 'Graduate' : 3, 'Post-Graduate' : 4, 'Doctorate' : 5}\n",
    "    df['Education_Level'] = df['Education_Level'].map(edu_mapping)\n",
    "    \n",
    "    # label \n",
    "    inc_mapping = {'Less than $40K' : 0, '$40K - $60K' : 1, '$60K - $80K' : 2, '$80K - $120K' : 3, '$120K +' : 4}\n",
    "    df['Income_Category'] = df['Income_Category'].map(inc_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = categorical_to_numerical(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df):\n",
    "    df.drop(['Marital_Status_Single','Gender_M','Marital_Status_Married','Card_Category','Dependent_count','missing_marital_status','missing_education_level','missing_income_category','Contacts_Count_12_mon','Credit_Limit','Months_on_book','Avg_Open_To_Buy','Total_Trans_Amt','Avg_Utilization_Ratio','Total_Amt_Chng_Q4_Q1','Total_Relationship_Count','is_36'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`Total_Trans_Ct`: This variable explains approximately 14% of the variation of bank attrition. Surprisingly, `Total_Trans_Amt` has a much lower percentage of variation of bank attrition explained (approximately 5%) even though both variables are highly correlated with each other. Therefore, we will keep `Total_Trans_Ct` and drop `Total_Trans_Amt`.\n",
    "\n",
    "The other variables not included in the first list do not explain a large percentage of variation in churn so we will drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2 Key Findings**\n",
    "\n",
    "`Credit_Limit`,`Avg_Open_To_Buy`,0.872472: Since Avg_Open_To_Buy is part of the equation of solving Credit_Limit, this correlation is not suprising. We will drop both features since both are not correlated highly with the target variable.\n",
    "\n",
    "`Total_Trans_Ct`,`Total_Trans_Amt`,0.771498: Since Total_Trans_Ct is more correlated with the target variable, we will keep it and drop Total_Trans_Amt.\n",
    "\n",
    "`Customer_Age`,`Months_on_book`,0.591667: We will drop Months_on_book.\n",
    "\n",
    "`Total_Revolving_Bal`,`Avg_Utilization_Ratio`,0.507283: We will just keep Total_Revolving_Bal since it is used in the equation to find Avg_Utilization_Ratio and it is more correlated with the target variable.\n",
    "\n",
    "`Avg_Open_To_Buy`,`Avg_Utilization_Ratio`,0.459306: We will  drop both features because Avg_Open_To_Buy isn't highly correlated with the target variable and Avg_Utilization_Ratio is correlated with another feature that is highly correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs= X_train.copy()\n",
    "X_train_fs = categorical_to_numerical(X_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features(X_train_fs)\n",
    "X_train_fs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "We will perform normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "scaler_fs = MinMaxScaler()\n",
    "X_train_fs = pd.DataFrame(scaler_fs.fit_transform(X_train_fs.values), columns=X_train_fs.columns, index=X_train_fs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df(scaler_type, df):\n",
    "    return pd.DataFrame(scaler_type.fit_transform(df.values), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs.to_csv('X_train_fs_scaled.csv',index=False)\n",
    "X_test_fs.to_csv('X_test_fs_scaled.csv',index=False)\n",
    "X_test.to_csv('X_test_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
