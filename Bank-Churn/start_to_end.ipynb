{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Churning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Objective:\n",
    "Identify key patterns and factors that determine attrition rate.\n",
    "\n",
    "## Data Model Objective:\n",
    "Build a model that predicts whether a customer is going to churn.\n",
    "\n",
    "### Dataset\n",
    "We will be using a credit card service company dataset from Kaggle (https://www.kaggle.com/sakshigoyal7/credit-card-customers).  \n",
    "\n",
    "### Why is identifying churn so important?\n",
    "In general, here are three ways a company can increase their revenue:\n",
    "1. Upsell to existing customers: Enhancing or improving products the products a customer is already buying.\n",
    "2. Acquire new customers\n",
    "3. Increase retention rate / lower churn\n",
    "\n",
    "Acquiring new customers costs much more than retaining them. As stated, we will focus on how a company can identify whether a customer can churn. Once a model that can identify the types of customers who are likely to churn or provide a probability of people who are likely to churn at any given time, business solutions such as issuing a retention campaign or promotion can be put into place to target those most prone to churning.\n",
    "\n",
    "\n",
    "## Questions I want to answer:\n",
    "\n",
    "- Classification\n",
    "    - What are the some specific behavior patterns that reveal potential churners? (anomaly detection)\n",
    "- ML Solution: Build predictive model\n",
    "- Business Solution: Determine high risk customers and target a retention campaign\n",
    "- Regression\n",
    "\n",
    "Probability they will churn in day X\n",
    "\n",
    "Expected time to churn - regression\n",
    "\n",
    "- To prevent churn: company based improvements, customer based marketing\n",
    "- In churn prediction:\n",
    "    - Be aware of class skew\n",
    "    - Consider both customer attributes and customer networks\n",
    "    - Interpretability, not just accuracy\n",
    "\n",
    "Stats question\n",
    "\n",
    "What is the probability someone will churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the most profitable customers...\n",
    "\n",
    "According to the New York Times, \"\" (https://www.nytimes.com/2013/11/08/business/economy/a-credit-card-rule-that-worked-for-consumers.html)\n",
    "\n",
    "#### in terms of average card utilization ratio?\n",
    "\n",
    "#### in terms of total transaction amount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "# Import the library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chisquare\n",
    "from sklearn import tree\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "1\n",
    "2\n",
    "3\n",
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns \n",
    "bank_churners = pd.read_csv('Data/BankChurners.csv').iloc[:,1:-2]\n",
    "\n",
    "# replace existing customer to 0 and attrited customer to 1\n",
    "bank_churners['Attrition_Flag'] = bank_churners['Attrition_Flag'].map({'Existing Customer' : 0, 'Attrited Customer' : 1})\n",
    "bank_churners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Churners Table\n",
    "\n",
    "- `Attrition_Flag`: Internal event (customer activity) variable - if the account is closed then 1 else 0\n",
    "- `Customer_Age`: Demographic variable - Customer's Age in Years\n",
    "- `Gender`: Demographic variable - M=Male, F=Female\n",
    "- `Dependent_count`: Demographic variable - Number of dependents\n",
    "- `Education_Level`: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n",
    "- `Marital_Status`: Demographic variable - Married, Single, Divorced, Unknown\n",
    "- `Income_Category`: Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n",
    "- `Card_Category`: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n",
    "- `Months_on_book`: Period of relationship with bank\n",
    "- `Total_Relationship_Count`: Total no. of products held by the customer\n",
    "- `Months_Inactive_12_mon`: No. of months inactive in the last 12 months\n",
    "- `Contacts_Count_12_mon`: No. of Contacts in the last 12 months\n",
    "- `Credit_Limit`: Credit Limit on the Credit Card\n",
    "- `Total_Revolving_Bal`: Total Revolving Balance on the Credit Card\n",
    "- `Avg_Open_To_Buy`: Open to Buy* Credit Line (Average of last 12 months)\n",
    "- `Total_Amt_Chng_Q4_Q1`: Change in Transaction Amount (Q4 over Q1)\n",
    "- `Total_Trans_Amt`: Total Transaction Amount (Last 12 months)\n",
    "- `Total_Trans_Ct`: Total Transaction Count (Last 12 months)\n",
    "- `Total_Ct_Chng_Q4_Q1`: Change in Transaction Count (Q4 over Q1) \n",
    "- `Avg_Utilization_Ratio`: Average Card Utilization Ratio***\n",
    "\n",
    "\n",
    "*Open-to-buy: The difference between the credit limit assigned to a cardholder account and the present balance on the account.\n",
    "\n",
    "***Average Card Utilization Ratio: Amount client owes divided by credit limit. (Total_Revolving_Bal / Credit_Limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of records and columns\n",
    "bank_churners.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churners.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at datatypes and missing values\n",
    "bank_churners.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values per feature\n",
    "for col in bank_churners.columns:\n",
    "    num_values = bank_churners[col].nunique()\n",
    "    print(col, \": \", num_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinguishing Categorical and Numeric Variables \n",
    "\n",
    "Although Total_Relationship_Count, Months_Inactive_12_mon, and Contacts_Count_12_mon are numeric features, we will classify them as categorical variables becuase they have a small amount of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(bank_churners.select_dtypes('object').columns)\n",
    "categorical_columns.extend(['Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon'])\n",
    "print(\"Categorical Features: \",categorical_columns)\n",
    "print()\n",
    "numerical_columns = list(bank_churners.select_dtypes(exclude=['object']).columns)\n",
    "numerical_columns = [i for i in numerical_columns if i not in ('Dependent_count','Attrition_Flag','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon')]\n",
    "print(\"Numerical Features: \", numerical_columns)\n",
    "print()\n",
    "print('Target Variable: Attrition_Flag')\n",
    "\n",
    "# includes attrition\n",
    "numerical_columns_attrition = list(bank_churners.select_dtypes(exclude=['object']).columns)\n",
    "numerical_columns_attrition = [i for i in numerical_columns if i not in ('Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Credit Card Users Who Attrited:', np.round(len(churned) / len(bank_churners) * 100, 2), '%')\n",
    "print('Percentage Unknown Variables:', np.round(len(bank_churners[bank_churners.replace('Unknown',np.nan).isna().any(axis=1)]) / len(bank_churners) * 100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of Credit Card Users Who Attrited:', np.round(len(bank_churners[bank_churners['Attrition_Flag']==1]) / len(bank_churners) * 100, 2), '%')\n",
    "print('Percentage Unknown Variables:', np.round(len(bank_churners[bank_churners.replace('Unknown',np.nan).isna().any(axis=1)]) / len(bank_churners) * 100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_churners.iloc[:,1:]\n",
    "y = bank_churners.iloc[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Number X_train dataset: \", X_train.shape)\n",
    "print(\"Number y_train dataset: \", y_train.shape)\n",
    "print(\"Number X_test dataset: \", X_test.shape)\n",
    "print(\"Number y_test dataset: \", y_test.shape)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(y_train,on=X_train.index).drop('key_0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_churned = train[train['Attrition_Flag'] == 0]\n",
    "churned = train[train['Attrition_Flag'] == 1]\n",
    "display(not_churned.describe())\n",
    "churned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.reset_index(drop=True)\n",
    "#y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "2.1 Handling Missing Data\n",
    "- Heatmap\n",
    "- Countplot\n",
    "- Venn Diagram\n",
    "- Observations\n",
    "\n",
    "2.1.1 Imputing Missing Values\n",
    "- Create missing column \n",
    "- Random Forest\n",
    "- Countplot\n",
    "- Observations\n",
    "\n",
    "2.2 Univariate Analysis (Descriptive)\n",
    "\n",
    "2.2.1 Categorical Data\n",
    "- Frequency Count\n",
    "- Frequency Count by Attrition\n",
    "- Frequency Percentage by Attrition\n",
    "- Observations\n",
    "\n",
    "2.2.2 Numerical Data\n",
    "- Distribution Plots\n",
    "- Distribution Plots by Attrition\n",
    "- Observations\n",
    "\n",
    "2.3 Bivariate Analysis (Correlation)\n",
    "\n",
    "2.3.1 Categorical vs. Categorical\n",
    "- Pointplot\n",
    "- Chi-Squared Test\n",
    "- Chi-Squared Test with Attrition\n",
    "- Observations\n",
    "\n",
    "2.3.2 Numerical vs. Numerical\n",
    "- Scatterplots \n",
    "- Spearman Correlation\n",
    "- Pearson Correlation\n",
    "- Observations\n",
    "\n",
    "2.3.3 Categorical vs. Numerical\n",
    "- Histogram\n",
    "- Normality Test\n",
    "- Wilcoxon-Rank Sum Test\n",
    "- Observations\n",
    "\n",
    "2.4 Key Findings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: Visualizing Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.replace('Unknown',np.nan).isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_cols = ['Education_Level','Marital_Status','Income_Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(16, 6))\n",
    "count=1\n",
    "for col in imputation_cols:\n",
    "    plt.subplot(1, 3, count)\n",
    "    plt.bar(x=X_train[col].value_counts().index,height=X_train[col].value_counts(normalize=True),color = 'blue')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(col)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Marital_Status` : The customers are mostly married or single, with a similar amount claiming they were divorced or 'unknown'.\n",
    "\n",
    "`Education_Level` : A plurality of credit card users claimed to have a least a graduate level. The unknown category ranks third in count.\n",
    "\n",
    "`Income_Category` : The 'Unknown\" category is the second to least most frequent category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing missing values\n",
    "unknown_education_level = len(X_train[X_train['Education_Level'] == 'Unknown']) / len(X_train)\n",
    "unknown_marital_status = len(X_train[X_train['Marital_Status']== 'Unknown']) / len(X_train)\n",
    "unknown_income_category = len(X_train[X_train['Income_Category']== 'Unknown']) / len(X_train)\n",
    "unknown_education_level_and_marital_status_only = len(X_train[(X_train['Education_Level']== 'Unknown') & (X_train['Marital_Status']== 'Unknown') & (X_train['Income_Category']!= 'Unknown')]) /len(X_train)\n",
    "unknown_education_level_and_income_category_only = len(X_train[(X_train['Education_Level']== 'Unknown') & (X_train['Marital_Status']!= 'Unknown') & (X_train['Income_Category']== 'Unknown')]) /len(X_train)\n",
    "unknown_income_category_and_marital_status_only = len(X_train[(X_train['Education_Level'] != 'Unknown') & (X_train['Marital_Status']== 'Unknown') & (X_train['Income_Category'] == 'Unknown')]) /len(X_train)\n",
    "unknown_total = len(X_train[(X_train['Education_Level']== 'Unknown') | (X_train['Marital_Status']== 'Unknown') | (X_train['Income_Category']== 'Unknown')]) /len(X_train)\n",
    "\n",
    "X_train_nan = X_train.replace('Unknown',np.nan)\n",
    "unknown_education_level_only = len(X_train[(X_train['Education_Level']== 'Unknown') & (X_train['Marital_Status']!= 'Unknown') & (X_train['Income_Category']!= 'Unknown')]) /len(X_train)\n",
    "unknown_income_category_only = len(X_train[(X_train['Education_Level']!= 'Unknown') & (X_train['Marital_Status']!= 'Unknown') & (X_train['Income_Category']== 'Unknown')]) /len(X_train)\n",
    "unknown_marital_status_only = len(X_train[(X_train['Education_Level']!= 'Unknown') & (X_train['Marital_Status']== 'Unknown') & (X_train['Income_Category']!= 'Unknown')]) /len(X_train)\n",
    "unknown_all = len(X_train[(X_train['Education_Level']== 'Unknown') & (X_train['Marital_Status']== 'Unknown') & (X_train['Income_Category']== 'Unknown')]) /len(X_train)\n",
    "\n",
    "\n",
    "# A: Education_Level\n",
    "# B: Marital_Status\n",
    "# C: Income_Category\n",
    "venn3(subsets = (round(unknown_education_level_only,3), round(unknown_marital_status_only,3),\n",
    "                 round(unknown_education_level_and_marital_status_only,3), round(unknown_income_category_only,3),\n",
    "                 round(unknown_education_level_and_income_category_only,3),round(unknown_income_category_and_marital_status_only,3),\n",
    "                 round(unknown_all,3)),\n",
    "                set_labels = ('Missing Education Level', 'Missing Marital Status', 'Missing Income Category'))\n",
    "plt.show()\n",
    "\n",
    "print('Percentage of Users with Missing Education Level: ', unknown_education_level*100)\n",
    "print('Percentage of Users with Missing Marital Status: ', unknown_marital_status*100)\n",
    "print('Percentage of Users with Missing Income Category: ', unknown_income_category*100)\n",
    "print('Total Percentage of Users with Missing Value(s): ', 100* len(X_train_nan[X_train_nan.isna().any(axis=1)]) / len(X_train_nan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Observations\n",
    "As we can see, 30% of the data contains one or more missing values. But, there's not much overlap between those missing values because only .01% of the data has all three missing features. About half of the missing data comes from records with missing education level, with income category coming in second and then marital status. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Imputing Missing Values Using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute Unknown Values with Most Frequent Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Unknown with random choice\n",
    "rf_X_train = X_train.copy().replace('Unknown',np.nan)\n",
    "\n",
    "def missing_to_random(df, variable):\n",
    "    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n",
    "    #df.loc[df[variable].isnull(),variable]=random_sample\n",
    "    return random_sample\n",
    "\n",
    "for col in imputation_cols:\n",
    "    rf_X_train.loc[rf_X_train[col].isnull(),col] = missing_to_random(rf_X_train,col)\n",
    "\n",
    "# create new x_train table with one-hot encoded categorical features\n",
    "rf_X_train_encoded = pd.get_dummies(rf_X_train,drop_first=True)\n",
    "rf_X_train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_X_test = X_test.copy().replace('Unknown',np.nan)\n",
    "\n",
    "for col in imputation_cols:\n",
    "    rf_X_test.loc[rf_X_test[col].isnull(),col] = missing_to_random(rf_X_test,col)\n",
    "\n",
    "# create new x_train table with one-hot encoded categorical features\n",
    "rf_X_test_encoded = pd.get_dummies(rf_X_test,drop_first=True)\n",
    "rf_X_test_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(16, 6))\n",
    "count=1\n",
    "for col in imputation_cols:\n",
    "    plt.subplot(1, 3, count)\n",
    "    plt.bar(x=rf_X_train[col].value_counts().index,height=rf_X_train[col].value_counts(normalize=True),color = 'navy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(col)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot: Includes randomly imputed values and existing values for features with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peform Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_X_train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf_train_encoded = rf_X_train_encoded.iloc[:,1:]\n",
    "y_rf_train_encoded = rf_X_train_encoded.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {\n",
    " 'max_depth': [10, 20, 30,],\n",
    " 'max_features': ['auto'],\n",
    " 'min_samples_leaf': [10,20,30],\n",
    " 'min_samples_split': [10,20,30],\n",
    " 'n_estimators': [50, 100, 150, 200]}\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_rf_X_train_encoded, y_rf_X_train_encoded)\n",
    "rf_random.best_params_ #(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Decision Matrix\n",
    "\n",
    "Each row represents a record from the bank_churners table. Each element within each row represent the decision node the decision tree has classified the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_missing = RandomForestClassifier(n_estimators=100,min_samples_split=20,min_samples_leaf=20,max_features='auto',max_depth=10,random_state=0)\n",
    "clf_missing.fit(X_rf_train_encoded, y_rf_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_matrix = clf_missing.apply(train)\n",
    "decision_matrix[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Proximity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_X_train_encoded[X_train_nan.isna().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize proximity matrix\n",
    "\n",
    "cols = len(X_train)\n",
    "missing_rows = rf_X_train_encoded[X_train_nan.isna().any(axis=1)].index.tolist()\n",
    "rows = len(missing_rows)\n",
    "num_trees = clf_missing.n_estimators\n",
    "proximity_matrix = [[0.0]*cols for i in range(rows)]\n",
    "\n",
    "# i is row (30% missing)\n",
    "r = 0\n",
    "for i in missing_rows:\n",
    "    # j columns (100%)\n",
    "    c=0\n",
    "    for j in np.arange(len(X_train)):\n",
    "        miss = enumerated_indices.get(i)\n",
    "        alss = enumerated_indices.get(j)\n",
    "        if miss != alss: # keeps i-j pair as 0\n",
    "            proximity_matrix[r][c] = np.count_nonzero(decision_matrix[miss] == decision_matrix[alss]) / num_trees\n",
    "        c+=1\n",
    "    r+=1\n",
    "\n",
    "# get specific rows that have unknown values\n",
    "# group the columns based on their value on missing data columns\n",
    "# sum each group's proximity matrix values (number of sums = number of groups) per 'unknown' value row\n",
    "# retrieve the biggest sum and assign unknown value as this key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_matrix[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Missing Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['missing_marital_status'] = np.where(X_train['Marital_Status'] == 'Unknown',1,0)\n",
    "X_train['missing_education_level'] = np.where(X_train['Education_Level'] == 'Unknown',1,0)\n",
    "X_train['missing_income_category'] = np.where(X_train['Income_Category'] == 'Unknown',1,0)\n",
    "missing_columns = ['missing_marital_status','missing_education_level','missing_income_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['missing_marital_status'] = np.where(X_test['Marital_Status'] == 'Unknown',1,0)\n",
    "X_test['missing_education_level'] = np.where(X_test['Education_Level'] == 'Unknown',1,0)\n",
    "X_test['missing_income_category'] = np.where(X_test['Income_Category'] == 'Unknown',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.extend(missing_columns)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Imputation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imp in imputation_cols:\n",
    "    print(X_train[imp].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows_dict_switch = dict(enumerate(missing_rows))\n",
    "missing_rows_dict = {}\n",
    "for keys,values in missing_rows_dict_switch.items():\n",
    "    missing_rows_dict[values]=keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_col(col_name):\n",
    "    # only use limited rows\n",
    "    #df = bank_churners\n",
    "    # denominator\n",
    "    num_estimators = num_trees\n",
    "    # categories including \"unknown\"\n",
    "    categories_with_unknown = X_train[col_name].value_counts().index\n",
    "    # proportion of categories without \"unknown\"\n",
    "    props_without_unknown = X_train[col_name].value_counts(normalize=True).drop('Unknown')\n",
    "    # instantiate list of imputed values for \"unknown\" rows\n",
    "    imputed_values = []\n",
    "    # categories not including \"unknown\"\n",
    "    categories_without_unknown = rf_X_train[col_name].value_counts(normalize=True).index\n",
    "    \n",
    "    # instantiate list of lists\n",
    "    # number of lists = number of categories including unknown\n",
    "    # within each list = row indices having category value\n",
    "    cols_list = []\n",
    "    # each list contains index of rows that have specific category\n",
    "    for cat in categories_with_unknown:\n",
    "        cols_list.append(X_train[X_train[col_name] == cat].index.values)\n",
    "  \n",
    "    # create dictionary, key: category name, value: indices with that category\n",
    "    dictionary = {categories_with_unknown[i] : cols_list[i] for i in range(len(categories_with_unknown))}\n",
    "    \n",
    "    # iterate through all unknown rows and assign value based on weighted avg\n",
    "    for unknown_row in dictionary['Unknown']:\n",
    "        # get proximity between each record and current unknown row\n",
    "        current_row = np.asarray(proximity_matrix[missing_rows_dict[unknown_row]])\n",
    "        \n",
    "        # compute a weighted sum score for each category\n",
    "        weighted_sums = []\n",
    "        for category_idx in categories_without_unknown:\n",
    "            # numerator: computes score for category using proximity scores of records in that category\n",
    "            num = sum(current_row[tuple([dictionary[category_idx]])])\n",
    "            # denominator: divide by number of random forest decision trees\n",
    "            den = num_estimators\n",
    "            # keeps track of sums for current category\n",
    "            weighted_sums.append(num /den * props_without_unknown[category_idx])\n",
    "        # appends category with greatest proximity to list\n",
    "        # this value will be the imputed value for the unknown record\n",
    "        imputed_values.append(categories_without_unknown[weighted_sums.index(max(weighted_sums))])\n",
    "    # return list of imputed values; one per unknown value in that column\n",
    "    return imputed_values\n",
    "                        \n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_values_list = []\n",
    "for col in imputation_cols:\n",
    "    # substitute nan values with imputed values\n",
    "    imputed_values = impute_missing_col(col)\n",
    "    X_train.loc[X_train[col].replace('Unknown',np.nan).isnull(),col]=imputed_values\n",
    "    imputed_values_list.append(imputed_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed Values Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(16, 6))\n",
    "count=1\n",
    "for col in imputation_cols:\n",
    "    plt.subplot(1, 3, count)\n",
    "    values = pd.Series(imputed_values_list[count-1])\n",
    "    plt.bar(x=values.value_counts().index,height=values.value_counts(),color = 'navy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(col)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Table Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(16, 6))\n",
    "count=1\n",
    "for col in imputation_cols:\n",
    "    plt.subplot(1, 3, count)\n",
    "    plt.bar(x=X_train[col].value_counts().index,height=X_train[col].value_counts(normalize=True),color = 'navy')\n",
    "    plt.xticks(rotation=45)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('bank_churner_imputed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed Values Observations\n",
    "\n",
    "**Histogram 1:**\n",
    "\n",
    "`Marital_Status`: The vast majority of people who were missing `Marital_Status` were imuted as being 'Married'. Only a small portion were categorized as 'Single' and none were categorized as 'Divorced'.\n",
    "\n",
    "`Education_Level`: Everyone was categorized as 'Graduate'.\n",
    "\n",
    "`Income_Category`: Everyone was categorized as 'Less than $40K'.\n",
    "\n",
    "**Histogram 2:**\n",
    "\n",
    "The order of proportions was preserved throughout all the columns with missing values, but the categories that ranked the highest increased even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.merge(y_train,on=X_train.index).drop('key_0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ms = []\n",
    "alpha = .05\n",
    "zero = X_train[X_train['missing_marital_status']==0]\n",
    "one = X_train[X_train['missing_marital_status']==1]\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_marital_status'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_ms.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_marital_status according to Chi-Squared Test: ', missing_ms)\n",
    "missing_ic = []\n",
    "alpha = .05\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_income_category'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_ic.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_income_category according to Chi-Squared Test: ', missing_ic)\n",
    "\n",
    "missing_el = []\n",
    "alpha = .05\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_education_level'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_el.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_education_level according to Chi-Squared Test: ', missing_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Univariate Analysis (Descriptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(y_train,on=X_train.index).drop('key_0',axis=1)\n",
    "test = X_test.merge(y_test,on=X_test.index).drop('key_0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_churned = train[train['Attrition_Flag'] == 0]\n",
    "churned = train[train['Attrition_Flag'] == 1]\n",
    "display(not_churned.describe())\n",
    "churned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,3,figsize=(12,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for cat in categorical_columns:\n",
    "    ar = sns.countplot(x=cat, data=X_train,ax=ax[count], palette='Blues')\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count by Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,3,figsize=(12,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for cat in categorical_columns:\n",
    "    ar = sns.countplot(x=cat, data=X,ax=ax[count], palette='Blues',hue='Attrition_Flag')\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    plt.subplots_adjust(hspace = 0.6)\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Percentage by Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,3,figsize=(12,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for cat in categorical_columns:\n",
    "    category_grouped = (X.groupby(['Attrition_Flag',cat]).size() / X.groupby(['Attrition_Flag']).size()).reset_index().rename({0:'percent'}, axis=1)\n",
    "    ar = sns.barplot(x=cat, hue='Attrition_Flag', y='percent', data=category_grouped,ax=ax[count], palette='Reds',dodge=True)\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    ar.legend_.remove()\n",
    "    count+=1 \n",
    "handles, labels = ax[count-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,title='Attrition_Flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** `Gender` : For both genders, the probability of churning is less than the probability of not churning. But it looks like males are less likely to churn than females. This means that gender may be a good predictor of churn.\n",
    "\n",
    "`Education_Level` : The distribution looks fairly similar for both attrited and non-attrited users. This means that education level may not be a good predictor of churn.\n",
    "\n",
    "`Marital_Status` : The distribution looks fairly similar for both attrited and non-attrited users. This means that marital status may not be a good predictor of churn.\n",
    "\n",
    "`Income_Category` : The distribution looks fairly similar for both attrited and non-attrited users. This means that income category may not be a good predictor of churn.\n",
    "\n",
    "`Card_Category` : The distribution looks fairly similar for both attrited and non-attrited users. This means that card category may not be a good predictor of churn.\n",
    "\n",
    "`Dependent_Count` : The distribution looks fairly similar for both attrited and non-attrited users. This means that dependent count may not be a good predictor of churn.\n",
    "\n",
    "** `Total_Relationship_Count` : The distribution for non-attrited users looks somewhat uniform out when the count is greater than 3 with count < 3 being the least probable. For attrited users, the probability of having a count 2 or 3 is the highest with the other categories being somewhat evenly less likely. This means that total relationship count may be a good predictor of churn.\n",
    "\n",
    "** `Months_Inactive_12_mon` : A majority of non-attrited users were only inactive between 1 to 3 months in the last 12 months. The distribution for attrited customers peaked at 3 with 2 being the second highest and the other counts less probable. This means that months inactive may be a good predictor of churn.\n",
    "\n",
    "** `Contacts_Count_12_mon` : The distribution of contacts_count_12_mon looks different for churned and non-churned users. This means Contact Count may be a good predictor of churn.\n",
    "\n",
    "`Missing_Marital_Status`: The distribution looks fairly similar for both attrited and non-attrited users. This means that missing marital status may not be a good predictor of churn.\n",
    "\n",
    "`Missing_Education_Level`: The distribution looks fairly similar for both attrited and non-attrited users. This means that missing education level may not be a good predictor of churn.\n",
    "\n",
    "`Missing_Income_Category`: The distribution looks fairly similar for both attrited and non-attrited users. This means that missing income category may not be a good predictor of churn.\n",
    "  \n",
    "\n",
    "** Represents categorical columns that may be a good predictor of churn based off their histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_hist = ['Gender','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for num in numerical_columns:\n",
    "    sns.distplot(a=X[num], ax=ax[count],kde=False)\n",
    "    count+= 1\n",
    "    sns.boxplot(x=num, data=X, ax=ax[count], palette='Blues')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plots by Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for n in numerical_columns:\n",
    "    sns.boxplot(y=n, data=X, ax=ax[count], palette='Blues',x='Attrition_Flag')\n",
    "    count+=1\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for num in numerical_columns:\n",
    "    for a in [churned[num], not_churned[num]]:\n",
    "        sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)\n",
    "    count+=1\n",
    "    \n",
    "orange_patch = mpatches.Patch(color='orange', label='Attrition_Flag=0')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Attrition_Flag=1')\n",
    "fig.legend(handles=[orange_patch,blue_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['Attrition_Flag'] > X['Attrition_Flag'].quantile(.75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_numerical_with_attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_numerical_hist = ['Total_Revolving_Bal','Total_Amt_Chng_Q4_Q1','Total_Trans_Amt','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1','Avg_Utilization_Ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data Observations\n",
    "\n",
    "`Customer_Age`: The distribution looks fairly similar for both attrited and non-attrited users. This means that education level may not be a good predictor of churn.\n",
    "\n",
    "`Months_on_book`: The distribution looks fairly similar for both attrited and non-attrited users. This means that Months on book may not be a good predictor of churn. The distribution looks somewhat normal but there is a big spike at around 36 months for both customer types.\n",
    "\n",
    "`Credit_Limit`: The distribution looks fairly similar for both attrited and non-attrited users. This means that credit limit may not be a good predictor of churn. Both distributions are highly right skewed, and there are some outliers towards the right tail that are prominent for both types of customers. A high percentage of both types also have a credit limit close to 0.\n",
    "\n",
    "** `Total_Revolving_Bal`: There is a high percentage of customers who have a balance close to 0 which is probably due their credit limit being close to 0. For churned customers, the distribution seems like an inverse gaussian distribution with the majority being at the tails. This may be because they were planning to leave (balance close to 0) or they were just inactive (balance on the right tail). In the non-churned distribution, more customers are clustered around the mean. But, it looks as if there are still many on the left and right tails. Also, almost every user who has a total_revolving_bal less than ~500 and greater than 0 is attrited. This means that total revolving balance may be a good predictor of churn.\n",
    "\n",
    "`Avg_Open_To_Buy`: The distribution looks fairly similar for both attrited and non-attrited users. This means that average open to buy may not be a good predictor of churn. Both distributions are skewed right meaning that most people have spent an amount close to their credit limit.\n",
    "\n",
    "** `Total_Amt_Chng_Q4_Q1` : Both distributions are clustered where the vlaue is less than 1 but non-attrited users are somewhat right skewed. Almost everyone who had a value greater than 1 was not attrited meaning those who spent more during the fourth quarter compared to the first quarter did not churn. This means that total amount change may be a good predictor of churn.\n",
    "\n",
    "** `Total_Trans_Amt` : It seems like there may be more than one mode (polymodal) for both distributions. Everyone who had an amount greater than approximately 11000 was non-attrited as well as those with a value about 3400- approximately 6000. This could indicate that our customers fall into some natural segmentation we can use. This means that total trans amount may be a good predictor of churn.\n",
    "\n",
    "** `Total_Trans_Ct` : Although it would seem intutive that this count is reflective of the total_trans_amt, the distributions seem to be different. The mean and standard deviation for non-churned customers is higher than churned customers. This makes sense. Everyone who had a count > ~90 was not attrited. This means that total trans count may be a good predictor of churn.\n",
    "\n",
    "** `Total_Ct_Chng_Q4_Q1` : Both distributions are heavily clustered under value < 1 and look somewhat normal. This means that a majority of customers, churned or not, made the almost the same amount of transactions in quarter 4 compared to quarter 1. This means that total count change may be a good predictor of churn.\n",
    "\n",
    "** `Avg_Utilization_Ratio` : The majority of churned customers have a ratio of 0 almost everyone who doesn't have that value is an outlier. This means that a majority of users made verys small purchases in regards to their credit limit. Churned customers are more likely to spend close to 0 compared to non-churned customers. This means that average utilization ratio may be a good predictor of churn.\n",
    "\n",
    "** Represents numerical columns that may be a good predictor of churn based off their histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Bivariate Analysis (Correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Categorical vs. Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cat in categorical_columns:\n",
    "    fig, ax = plt.subplots(2,6,figsize=(20,6))\n",
    "    ax = ax.ravel()\n",
    "    count = 0\n",
    "    for cat1 in categorical_columns:\n",
    "        if cat1 != cat:\n",
    "            category_grouped = (X_train.groupby([cat,cat1]).size() / X_train.groupby([cat]).size()).reset_index().rename({0:'percent'}, axis=1)\n",
    "            ar = sns.pointplot(x=cat1, hue=cat, y='percent', data=category_grouped,ax=ax[count], palette='Spectral',dodge=True)\n",
    "            plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "            plt.subplots_adjust(hspace = 0.8)\n",
    "            ar.legend_.remove()\n",
    "            count+=1 \n",
    "\n",
    "    handles, labels = ax[count-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels,title=cat)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test\n",
    "\n",
    "We want to see if these observations are statistically significant. We will compute a chi-squared test to see whether there is an association between any pair of categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_variables = []\n",
    "alpha = .05\n",
    "enumerated = list(itertools.combinations(categorical_columns, r=2))\n",
    "\n",
    "for pair in enumerated:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train[pair[0]],X_train[pair[1]]))[1]\n",
    "    if p_val < alpha:\n",
    "        associated_categorical_variables.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with each other according to Chi-Squared Test: ')\n",
    "associated_categorical_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test and Countplot Observations\n",
    "    \n",
    "`Income_Category` vs. `Gender` : Every female claim they make less than \\\\$40K or \\\\$40K-\\\\$60K. In contrast, most males claim they make \\\\$80K-\\\\$120K, with \\\\$60K-\\\\$80K coming in second. Females tend to make less money than males.\n",
    "\n",
    "`Gender` vs. `Card_Category`: Blue card holders are comprised of more females than male. But, for every other card category the reverse is true.\n",
    "\n",
    "`Gender` vs. `Contacts_Count_12_mon`: Credit card users who were in contact with 5 people from the credit card company were majority female. Credit card users who were in contact with less than 5 or greater than 5 people from the credit company were majority male.\n",
    "\n",
    "`Marital_Status` vs. `Card_Category`: People in the blue category are more likely to be married than people in the other categories. People in the blue category are less likely to be single than people in the other categories.\n",
    "\n",
    "`Marital_Status` vs. `Dependent_count`: People who have dependents are more likely to be married than single. People who have zero dependents are somewhat equally likely to be married or single.\n",
    "\n",
    "`Card_Category` vs. `Income_Category` : The distribution of incomes for each card category looks vastly different. Blue card holders have more people who make less than \\$40K compared to other card categories. People who have more income have a higher card level.\n",
    "\n",
    "`Income_Category` vs. `Dependent_count`: People who make $120K+ are more likely to have two dependents than any other income level.\n",
    "\n",
    "`Card_Category` vs. `Total_Relationship_Count` : Platinum members tend to buy only one to three products. The lower the card status, the more spread out the total relationship count distribution is from one to six products. This means there may be a relationship between card category and total relationship count.\n",
    "\n",
    "`Dependent_count` vs. `Total_Relationship_Count`: People who have less dependents are more likely to purchase more products with the credit card company.\n",
    "\n",
    "`Dependent_count` vs. `Contacts_Count_12_mon`: People who had more contact with the company are more likely to have more dependents.\n",
    "\n",
    "`Total_Relationship_Count` vs. `Months_Inactive_12_mon` : The distribution of products owned by a customer is very different for customers who were inactive 0 months out of the last twelve months compared to the rest of customers. People who were active for all months of the past year had a different relationship count pattern than any other type of consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ms = []\n",
    "alpha = .05\n",
    "zero = X_train[X_train['missing_marital_status']==0]\n",
    "one = X_train[X_train['missing_marital_status']==1]\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_marital_status'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_ms.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_marital_status according to Chi-Squared Test: ', missing_ms)\n",
    "missing_ic = []\n",
    "alpha = .05\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_income_category'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_ic.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_income_category according to Chi-Squared Test: ', missing_ic)\n",
    "\n",
    "missing_el = []\n",
    "alpha = .05\n",
    "\n",
    "for pair in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train['missing_education_level'], X_train[pair]))[1]\n",
    "    if p_val < alpha:\n",
    "        missing_el.append(pair)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with missing_education_level according to Chi-Squared Test: ', missing_el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Target vs. Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "fig, ax = plt.subplots(4,3,figsize=(20,6))\n",
    "ax = ax.ravel()\n",
    "for cat1 in categorical_columns:\n",
    "    category_grouped = (X.groupby(['Attrition_Flag',cat1]).size() / X.groupby(['Attrition_Flag']).size()).reset_index().rename({0:'percent'}, axis=1)\n",
    "    ar = sns.pointplot(x=cat1, hue='Attrition_Flag', y='percent', data=category_grouped,ax=ax[count], palette='Spectral',dodge=True)\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    plt.subplots_adjust(hspace = 0.8)\n",
    "    ar.legend_.remove()\n",
    "    count+=1 \n",
    "handles, labels = ax[count-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,title='Attrition_Flag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_variables_with_target = []\n",
    "alpha = .05\n",
    "\n",
    "for cat in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X[cat],X['Attrition_Flag']))[1]\n",
    "    if p_val < alpha:\n",
    "        associated_categorical_variables_with_target.append(cat)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', pair, 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with Attrition_Flag: ')\n",
    "associated_categorical_variables_with_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared Test and Countplot Results\n",
    "\n",
    "`Gender`: People who attrite are more likely to be female than male.\n",
    "\n",
    "`Income_Category`: People who attrite are less likely to be married.\n",
    "\n",
    "`Total_Relationship_Count`: People who bought more products are less likely to attrite.\n",
    "\n",
    "`Months_Inactive_12_mon`: As months inactive increases, so does the probability of attriting.\n",
    "\n",
    "`Contacts_Count_12_mon`: Consumers who were in contact with more people in the credit card company are more likely to attrite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Numerical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Customer_Age'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [not_churned[col], churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Months_on_book'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [not_churned[col], churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Credit_Limit'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [not_churned[col], churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Total_Revolving_Bal'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [not_churned[col], churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Avg_Open_To_Buy'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Total_Trans_Amt'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Total_Trans_Ct'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Total_Amt_Chng_Q4_Q1'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Total_Ct_Chng_Q4_Q1'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "col = 'Avg_Utilization_Ratio'\n",
    "\n",
    "for num in numerical_columns:\n",
    "    if num != col:\n",
    "        sns.scatterplot(y=num,x=col,ax=ax[count],data=X,hue='Attrition_Flag')\n",
    "        count+=1\n",
    "    \n",
    "for a in [churned[col], not_churned[col]]:\n",
    "    sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Credit_Limit` Observations\n",
    "\n",
    "`Avg_Open_To_Buy` : There is a positive linear relationship between the two variables with correlation close to 1.\n",
    "\n",
    "`Avg_Utilization_Ratio` : There is a negative relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Months_on_book` Observations\n",
    "\n",
    "`Customer_Age` : There seems to be a linear postive relationship between customer_age and months_on_book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Total_Trans_Ct` Observations\n",
    "\n",
    "`Total_Trans_Amt` : There is a positive relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Avg_Open_To_Buy` Observations\n",
    "\n",
    "`Credit_Limit` : Positive linear relationship with correlation close to 1.\n",
    "    \n",
    "`Avg_Utilization_Ratio` : Negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Total_Trans_Amt` Observations\n",
    "\n",
    "`Total_Trans_Ct` : There is a positive relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Total_Amt_Chng_Q4_Q1` Observations\n",
    "\n",
    "`Total_Trans_Amt` : Almost everyone who was attrited had a trans amount less than than ~12000 and chng less than 1, with very few outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Total_Revolving_Bal` Observations\n",
    "\n",
    "`Avg_Utilization_Ratio` : There is a positive relationship. \n",
    "\n",
    "`Total_Trans_Amt` : People who had 0 balance and trans amount < 12500 were attrited and those who had 0 balance and trans amount >= 12500 were non-attrited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Total_Ct_Chng_Q4_Q1` Observations\n",
    "\n",
    "`Total_Amt_Chng_Q4_Q1` : There is a positive relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Avg_Utilization_Ratio` Observations\n",
    "\n",
    "`Credit_Limit` : There is a negative relationship.\n",
    "    \n",
    "`Total_Revolving_Bal` : There is a positive relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Customer_Age` Insights\n",
    "\n",
    "`Months_on_book` : There seems to be a linear postive relationship between customer_age and months_on_book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot Observations\n",
    "\n",
    "`Credit_Limit` vs. `Avg_Open_To_Buy` : There is a positive linear relationship between the two variables with correlation close to 1.\n",
    "\n",
    "`Credit_Limit` vs. `Avg_Utilization_Ratio` : There is a negative relationship between the two variables. \n",
    "\n",
    "`Months_on_book` vs. `Customer_Age` : There seems to be a linear postive relationship between customer_age and months_on_book.\n",
    "\n",
    "`Total_Trans_Ct` vs `Total_Trans_Amt` : There is a positive relationship between the two variables.\n",
    "\n",
    "`Avg_Open_To_Buy` vs. `Credit_Limit` : Positive linear relationship with correlation close to 1.\n",
    "\n",
    "`Avg_Open_To_Buy` vs. `Avg_Utilization_Ratio` : Negative relationship.\n",
    "\n",
    "`Total_Trans_Amt` vs. `Total_Trans_Ct` : There is a positive relationship.\n",
    "\n",
    "`Total_Amt_Chng_Q4_Q1` vs. `Total_Trans_Amt` : Almost everyone who was attrited had a trans amount less than than ~12000 and chng less than 1, with very few outliers.\n",
    "\n",
    "`Total_Revolving_Bal` vs. `Avg_Utilization_Ratio` : There is a positive relationship. \n",
    "\n",
    "`Total_Revolving_Bal` vs. `Total_Trans_Amt` : People who had 0 balance and trans amount < 12500 were attrited and those who had 0 balance and trans amount >= 12500 were non-attrited. \n",
    "\n",
    "`Total_Ct_Chng_Q4_Q1` vs. `Total_Amt_Chng_Q4_Q1` : There is a positive relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps\n",
    "\n",
    "We will be assessing Spearman correlation to account for non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr(method='spearman'), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.corr(method='pearson'), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be too big of a difference in correlation heatmpas so will use Spearman correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = X.corr('spearman').unstack()\n",
    "\n",
    "s_sorted = s.sort_values(kind=\"quicksort\",ascending=False)\n",
    "\n",
    "associated_numerical_positive = s_sorted[(s_sorted.values < 1) & (s_sorted.values >.50) & (s_sorted.index.get_level_values(0) !='Attrition_Flag') & (s_sorted.index.get_level_values(1) !='Attrition_Flag')]\n",
    "print(associated_numerical_positive)\n",
    "\n",
    "associated_numerical_negative = s_sorted[(s_sorted.values < 0) & (s_sorted.values <-.5) & (s_sorted.index.get_level_values(0) !='Attrition_Flag') & (s_sorted.index.get_level_values(1) !='Attrition_Flag')]\n",
    "print(associated_numerical_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical vs. Numerical Correlated Features:\n",
    "\n",
    "`Credit_Limit` vs. `Avg_Open_To_Buy`: 0.934062 There is a positive linear relationship between the two variables with correlation close to 1. This indicates that people who have a higher credit limit have more unspent credit.\n",
    "\n",
    "`Total_Trans_Ct` vs. `Total_Trans_Amt`: 0.878350 There is a positive relationship between the two variables. Those who spend a large amount of credit make more transactions.\n",
    "\n",
    "`Customer_Age` vs. `Months_on_book`: 0.769199 There is a linear postive relationship between customer_age and months_on_book.Older customers have a longer relationship with the company.\n",
    "\n",
    "`Total_Revolving_Bal` vs. `Avg_Utilization_Ratio`: 0.712238 Total_Revolving Balance is used in the equation for Avg_Utilization. People who spend more credit have a higher average utilization ratio.\n",
    "\n",
    "`Avg_Utilization_Ratio` vs`Avg_Open_To_Buy`: -0.677721 People who spend less of their credit limit have more credit left to spend.\n",
    "\n",
    "\n",
    "### Less Correlated Features:\n",
    "\n",
    "`Avg_Utilization_Ratio` vs`Credit_Limit` -0.416959: People who spend less of their credit have more credit.\n",
    "\n",
    "`Total_Relationship_Count` vs`Total_Trans_Amt` -0.279113: People who have a longer relationship with the bank spend less credit.\n",
    "\n",
    "`Total_Relationship_Count` vs`Total_Trans_Ct` -0.226808: People who have a longer relationship with the bank make less transactions.\n",
    "\n",
    "`Total_Trans_Ct` vs`Contacts_Count_12_mon` -0.168413: People who make more transactions were inactive for less months compared to those who made less transactions.\n",
    "\n",
    "`Contacts_Count_12_mon` vs`Total_Trans_Amt` -0.167372: People who spend more credit had less contact with the bank.\n",
    "\n",
    "`Avg_Open_To_Buy` vs`Total_Revolving_Bal` -0.154165: People who spend more of their credit have less credit to spend.\n",
    "\n",
    "`Total_Ct_Chng_Q4_Q1` vs. `Total_Amt_Chng_Q4_Q1`: 0.301981 People who spend more in the fourth quarter compared to the first make more transactions in the fourth quarter than the first.\n",
    "\n",
    "`Total_Trans_Ct` vs. `Total_Ct_Chng_Q4_Q1`: 0.233448: People who make more transactions in general spend their money more in the fourth quarter compared to the first. People who make less transactions do not increase the number of transactions made in the fourth quarter compared to the first.\n",
    "\n",
    "`Total_Ct_Chng_Q4_Q1` vs. `Total_Trans_Amt`: 0.222688: People who make more transactions in the fourth quarter compared to the first spend more credit in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_numerical_with_attrition = X.corr('spearman')['Attrition_Flag']\n",
    "correlated_numerical_with_attrition = correlated_numerical_with_attrition[(correlated_numerical_with_attrition.values < 1) & (abs(correlated_numerical_with_attrition.values) >.20)]#.sort_values(ascending=False)\n",
    "correlated_numerical_with_attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr('spearman')['Attrition_Flag'].sort_values()**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical vs. Target Correlation Observations:\n",
    "\n",
    "`Total_Trans_Ct` -0.376115: People who make less transactions are more likely to attrite.\n",
    "\n",
    "`Total_Ct_Chng_Q4_Q1` -0.312059: People who make more transactions in the fourth quarter compared to the first quarter are less likely to attrite.\n",
    "\n",
    "`Total_Revolving_Bal` -0.240551: People who spend more credit are less likely to attrite.\n",
    "\n",
    "`Avg_Utilization_Ratio` -0.240385: People who spend a larger proportion of their credit limit are less likely to attrite.\n",
    "\n",
    "`Total_Trans_Amt` -0.223782: People who spend more credit are less likely to attrite.\n",
    "\n",
    "`Contacts_Count_12_mon` 0.189038: People who have more contact with the bank in the last twelve months are more likely to attrite.\n",
    "\n",
    "`Months_Inactive_12_mon` 0.171839: People who were inactive for more months of the last year are more likely to attrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .05\n",
    "for num in numerical_columns:\n",
    "    combined_stat = stats.shapiro(X_train[num])[1]\n",
    "    if combined_stat < alpha:\n",
    "        print( num, 'Distribution IS NOT normally distributed with p-value of ', combined_stat)\n",
    "    else:\n",
    "        print(num, 'Distribution IS normally distributed with p-value of ', combined_stat)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Wilcoxon Rank-Sum Test\n",
    "\n",
    "Since not normal, we will compute wilcoxon rank-sum test to see if medians are same for attrited variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_columns = []\n",
    "for col_name in numerical_columns:\n",
    "    pval = stats.ranksums(churned[col_name], not_churned[col_name]).pvalue\n",
    "    if pval < alpha:\n",
    "        related_columns.append(col_name)\n",
    "print('Numerical Features with different medians for churned and non-churned customers: ')\n",
    "related_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_numerical_with_attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Correlation Results with Wilcoxon Rank-Sum Results\n",
    "\n",
    "Not Surprising Results:\n",
    "\n",
    "`Avg_Utilization_Ratio`,`Total_Amt_Chng_Q4_Q1`, `Total_Ct_Chng_Q4_Q1`,`Total_Trans_Ct`, `Total_Trans_Amt` and `Total_Revolving_Bal`: These attributes were important in both statistics for determining attrition. People who increasingly spend more of their credit limit within the last year are less likely to attrite.\n",
    "\n",
    "`Credit_Limit`: The difference in median values for churned and non-churned customers for this attribute was statitically significant, but did not have a strong correlation with attrition rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_and_numerical_variables = []\n",
    "\n",
    "for cat in missing_columns:\n",
    "    for num in numerical_columns:\n",
    "        zero = X_train[X_train[cat] == 0]\n",
    "        one = X_train[X_train[cat] == 1]\n",
    "        pval = stats.kruskal(zero[num].values,one[num].values).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1))\n",
    "\n",
    "cat = 'Gender'\n",
    "for num in numerical_columns:\n",
    "    female = X_train[X_train[cat] == 'F']\n",
    "    male = X_train[X_train[cat] == 'M']\n",
    "    pval = stats.kruskal(female[num].values,male[num].values).pvalue\n",
    "    if pval < alpha:\n",
    "       # print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "        associated_categorical_and_numerical_variables.append((cat,num,1))\n",
    "   \n",
    "three_categories = ['Marital_Status']\n",
    "for cat in three_categories:\n",
    "    cats = X_train[cat].value_counts().index\n",
    "    for num in numerical_columns:\n",
    "        cat1 = X_train[X_train[cat] == cats[0]][num].values\n",
    "        cat2 = X_train[X_train[cat] == cats[1]][num].values\n",
    "        cat3 = X_train[X_train[cat] == cats[2]][num].values\n",
    "        pval = stats.kruskal(cat1,cat2,cat3).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1))\n",
    "        \n",
    "        \n",
    "four_categories = ['Card_Category']\n",
    "for cat in four_categories:\n",
    "    cats = X_train[cat].value_counts().index\n",
    "    for num in numerical_columns:\n",
    "        cat1 = X_train[X_train[cat] == cats[0]][num].values\n",
    "        cat2 = X_train[X_train[cat] == cats[1]][num].values\n",
    "        cat3 = X_train[X_train[cat] == cats[2]][num].values\n",
    "        cat4 = X_train[X_train[cat] == cats[3]][num].values\n",
    "        pval = stats.kruskal(cat1,cat2,cat3,cat4).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1))\n",
    "            \n",
    "five_categories = ['Income_Category']\n",
    "for cat in five_categories:\n",
    "    cats = X_train[cat].value_counts().index\n",
    "    for num in numerical_columns:\n",
    "        cat1 = X_train[X_train[cat] == cats[0]][num].values\n",
    "        cat2 = X_train[X_train[cat] == cats[1]][num].values\n",
    "        cat3 = X_train[X_train[cat] == cats[2]][num].values\n",
    "        cat4 = X_train[X_train[cat] == cats[3]][num].values\n",
    "        pval = stats.kruskal(cat1,cat2,cat3,cat4).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1))\n",
    "            \n",
    "six_categories = ['Dependent_count','Total_Relationship_Count','Education_Level']\n",
    "\n",
    "for cat in six_categories:\n",
    "    cats = X_train[cat].value_counts().index\n",
    "    for num in numerical_columns:\n",
    "        cat1 = X_train[X_train[cat] == cats[0]][num].values\n",
    "        cat2 = X_train[X_train[cat] == cats[1]][num].values\n",
    "        cat3 = X_train[X_train[cat] == cats[2]][num].values\n",
    "        cat4 = X_train[X_train[cat] == cats[3]][num].values\n",
    "        cat5 = X_train[X_train[cat] == cats[4]][num].values\n",
    "        cat6 = X_train[X_train[cat] == cats[5]][num].values\n",
    "        pval = stats.kruskal(cat1,cat2,cat3,cat4,cat5,cat6).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1))    \n",
    "            \n",
    "seven_categories = ['Months_Inactive_12_mon','Contacts_Count_12_mon' ]\n",
    "\n",
    "for cat in seven_categories:\n",
    "    cats = X_train[cat].value_counts().index\n",
    "    for num in numerical_columns:\n",
    "        cat1 = X_train[X_train[cat] == cats[0]][num].values\n",
    "        cat2 = X_train[X_train[cat] == cats[1]][num].values\n",
    "        cat3 = X_train[X_train[cat] == cats[2]][num].values\n",
    "        cat4 = X_train[X_train[cat] == cats[3]][num].values\n",
    "        cat5 = X_train[X_train[cat] == cats[4]][num].values\n",
    "        cat6 = X_train[X_train[cat] == cats[5]][num].values\n",
    "        cat7 = X_train[X_train[cat] == cats[6]][num].values\n",
    "        pval = stats.kruskal(cat1,cat2,cat3,cat4,cat5,cat6,cat7).pvalue\n",
    "        if pval < alpha:\n",
    "            #print('There is a relationship between categorical variable', cat, 'and numerical variable', num)\n",
    "            associated_categorical_and_numerical_variables.append((cat,num,1)) \n",
    "\n",
    "associated_categorical_and_numerical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "val_1=collections.Counter([x for (x,y,z) in associated_categorical_and_numerical_variables])\n",
    "val_2=collections.Counter([y for (x,y,z) in associated_categorical_and_numerical_variables])\n",
    "print('Number of Categorical: ',len(categorical_columns))\n",
    "print('Number of Numerical:', len(numerical_columns))\n",
    "display(val_1)\n",
    "display(val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the number, the more features it is correlated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(associated_categorical_and_numerical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_cat = pd.DataFrame(associated_categorical_and_numerical_variables, columns =['Categorical', 'Numerical', 'Relationship']) \n",
    "pivot_num_cat = df_num_cat.pivot('Categorical', 'Numerical','Relationship') \n",
    "\n",
    "sns.heatmap(pivot_num_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for num in numerical_columns:\n",
    "        sns.boxenplot(x=num, data=X_train, ax=ax[count], palette='Blues',y='Education_Level')\n",
    "        plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "        plt.subplots_adjust(hspace = 0.8)\n",
    "        if count !=0:\n",
    "            plt.axis(\"off\")\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Education_Level: There are not numerical variables that are associated with education_level meaning education_level doesn't play a significant role in determiging churned and non-churned customers. This is surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(5,3,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for cat,num,one in associated_categorical_and_numerical_variables[:15]:\n",
    "        sns.violinplot(y=num, data=X_train, ax=ax[count], palette='Blues',x=cat)\n",
    "        plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "        plt.subplots_adjust(hspace = 0.8)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gender` and `Credit_Limit` : Females generally have lower credit limit than males. \n",
    "\n",
    "`Gender` and `Avg_Open_To_Buy` : Although males and females tend to spend the same amount of credit, females have less credit left because of their lower credit limit.\n",
    "\n",
    "`Gender` and `Total_Trans_Amt` and `Total_Trans_Ct`: Females generally spend a little bit more and make more transactions than males.\n",
    "\n",
    "`Gender` and `Avg_Utilization_Ratio` : Males have a lower utilization ratio because they spend slightly less credit than females but also have a much higher credit limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for cat,num,one in associated_categorical_and_numerical_variables[15:30]:\n",
    "        sns.violinplot(y=num, data=X_train, ax=ax[count], palette='Blues',x=cat)\n",
    "        plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "        plt.subplots_adjust(hspace = 0.8)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Card_Category` vs. `Credit_Limit` : Those who are in the Blue category have a much lower credit_limit than those in Gold, Silver, and Platinum with Platinum having the highest median.\n",
    "\n",
    "`Card_Category` vs. `Avg_Utilization_Ratio` : Those in the blue category are more likely spend an amount closer to their credit limit than other card categories, but keep in mind their credit limits are much lower than other card categories. There is still a substantial population of blue card holders who don't use their credit cards.\n",
    "\n",
    "`Customer_Age` vs. `Income_Category`: Those who make 120K+ tend to be older than people with less income.\n",
    "\n",
    "`Months_on_book` vs. `Income_Category`: The spike at 36 months is prevalant for all people of different income.\n",
    "\n",
    "`Credit_Limit` vs. `Income_Category`: People who make 80K and more tend to have a higher credit limit than those who make less.\n",
    "\n",
    "`Avg_Open_To_Buy` vs. `Income_Category`: People who make 80K and more tend have more credit not spent than those who make less.\n",
    "\n",
    "`Avg_Utilization_Ratio` vs. `Income_Category`: People who make 60K or more tend to have a utilization_ratio closer to 0, meaning they don't spend close to their credit limit.\n",
    "\n",
    "`Avg_Utilization_Ratio` vs. `Income_Category`: Adults who are very young or very old have less dependents than people who are middle aged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for cat,num,one in associated_categorical_and_numerical_variables[30:45]:\n",
    "        sns.violinplot(y=num, data=X_train, ax=ax[count], palette='Blues',x=cat)\n",
    "        plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "        plt.subplots_adjust(hspace = 0.8)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Months_on_book` vs `Dependent_count` : People who have less dependents tend to be older.\n",
    "\n",
    "`Total_Relationship_Amt` vs. `Total_Trans_Amt`: People who bought less products from the credit card company tend to spend more.\n",
    "\n",
    "`Total_Relationship_Ct` vs. `Total_Trans_Amt`: People who bought less products from the credit card company tend to have more transactions.\n",
    "\n",
    "`Months_Inactive_12_mon` vs. `Customer_Age`: Older people tend to be more active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,3,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count = 0\n",
    "for cat,num,one in associated_categorical_and_numerical_variables[45:]:\n",
    "        sns.violinplot(y=num, data=X_train, ax=ax[count], palette='Blues',x=cat)\n",
    "        plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "        plt.subplots_adjust(hspace = 0.8)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Months_on_book` vs. `Months_Inactive_12_mon`: People who were inactive for less months tend to have a longer relationship with the credit card company and did not have a spike at 36 months. But, those who were inactive for a longer time tend to not have as large of a spike at 36 months either.\n",
    "\n",
    "`Months_Inactive_12_mon`: We can group people into inactive statuses of 0 months, 1-3 months, and 4-6 months. Credit_Limit, Total_Reovlving_Bal, Total_Trans_Amt, Total_Trans_Ct, and Avg-Utilization_Ratio.\n",
    "\n",
    "`Avg_Utilization_Ratio` vs. `Contacts_Count_12_mon`: Customers who spoke to more contacts have a lower median Avg-Utilization_ratio compared to those who spoke to less contacts, meaning they spent a bigger portion of their credit limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are there so many people who have a `Months_on_book` value of 36?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_36 = X[X['Months_on_book'] == 36]\n",
    "months_not_36 = X[X['Months_on_book'] != 36]\n",
    "display(months_36.describe())\n",
    "display(months_not_36.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['is_36'] = np.where(X['Months_on_book'] == 36, 1,0)\n",
    "\n",
    "# can't use no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_variables_with_month = []\n",
    "alpha = .05\n",
    "\n",
    "for cat in categorical_columns:\n",
    "    p_val = stats.chi2_contingency(pd.crosstab(X_train[cat],X_train['is_36']))[1]\n",
    "    if p_val < alpha:\n",
    "        associated_categorical_variables_with_month.append(cat)\n",
    "        #print('We REJECT the null hypothesis. There IS an association between', 'with p-value of ',p_val)\n",
    "    #else:\n",
    "       # print('We ACCEPT the null hypothesis. There IS NOT an association between', pair, 'with p-value of ',p_val)\n",
    "print('Categorical Variables associated with Months_on_book = 36: ')\n",
    "associated_categorical_variables_with_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "\n",
    "for cat1 in associated_categorical_variables_with_month:\n",
    "    category_grouped = (X_train.groupby(['is_36',cat1]).size() / X_train.groupby(['is_36']).size()).reset_index().rename({0:'percent'}, axis=1)\n",
    "    ar = sns.pointplot(x=cat1, hue='is_36', y='percent', data=category_grouped,ax=ax[count], palette='Spectral',dodge=True)\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    count+=1 \n",
    " \n",
    "fig, ax = plt.subplots(2,2,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for cat1 in associated_categorical_variables_with_month:\n",
    "    category_grouped = (X_train.groupby(['is_36',cat1]).size() / X_train.groupby([cat1]).size()).reset_index().rename({0:'percent'}, axis=1)\n",
    "    ar = sns.pointplot(x='is_36', hue=cat1, y='percent', data=category_grouped,ax=ax[count], palette='Spectral',dodge=True)\n",
    "    plt.setp(ax[count].get_xticklabels(), rotation=45)\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "`Gender`: The gender distribution is more evenly distributed when months-on-book is not equal to 36; in contrast the gender distribution is more skewed towards females when months-on-book equals 36.\n",
    "\n",
    "`Income_Category`: As income increases, the likelihood of having a months-on-book value = 36 decreases.\n",
    "\n",
    "`Dependent_count`: People who have more dependents are more likely to have a months-on-book value of 36.\n",
    "\n",
    "`Months_Inactive_12_mon`: The more inactive a person is, the more likely they have a months-on-book value of 36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,5,figsize=(20,12))\n",
    "ax = ax.ravel()\n",
    "count=0\n",
    "for num in numerical_columns:\n",
    "    for a in [months_36[num], months_not_36[num]]:\n",
    "        sns.distplot(a, ax=ax[count],norm_hist=True,kde=False)\n",
    "    count+=1\n",
    "    \n",
    "orange_patch = mpatches.Patch(color='orange', label='Attrition_Flag=0')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Attrition_Flag=1')\n",
    "fig.legend(handles=[orange_patch,blue_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_columns = []\n",
    "for col_name in numerical_columns:\n",
    "    pval = stats.ranksums(months_36[col_name], months_not_36[col_name]).pvalue\n",
    "    if pval < alpha:\n",
    "        related_columns.append(col_name)\n",
    "print('Numerical Features with different medians for customers with Months_on_book = 36 and Months_on_book != 36: ')\n",
    "related_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ranksums(months_36['Attrition_Flag'], months_not_36['Attrition_Flag']).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Since the p-value is greater than .05, we fail to reject the null hypothesis. There is no relationship betwen attrition flag and months_on_book=36.\n",
    "\n",
    "Having a months_on_book value of 36 is not associated with any numerical variable nor the target variable. This means peoples' behavior is not associated with haivng a months-on-book value = 36, but rather their demographic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Feature Engineering\n",
    "\n",
    "3.1 Train-Test Split\n",
    "\n",
    "3.2 Handle Categorical Data\n",
    "- turn into numerical data\n",
    "- one hot encode, label encode\n",
    "\n",
    "3.3 Feature Scaling (Numerical Data)\n",
    "\n",
    "3.4 Feature Selection\n",
    "- test different methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_preprocessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_preprocessed.csv')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_scaled.csv')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "# baseline with smote after splittling kfolds\n",
    "kf = KFold(n_splits=5,shuffle=False)\n",
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              RandomForestClassifier())\n",
    "#cross_val_score(imba_pipeline, X_train, y_train, scoring='recall', cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Feature Selection\n",
    "- Numerical vs. Target (Correlation)\n",
    "- Numerical vs. Numerical (Correlation)\n",
    "- Categorical vs. Target (Chi-Squared)\n",
    "- Categorical vs. Categorical (Chi-Squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Numerical vs. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_numerical_hist\n",
    "# associated numerical variables with target based off visual histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_numerical_with_attrition.sort_values()\n",
    "# numerical features most correlated with attrition flag (less correlated features not included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Total_Trans_Ct`: This variable explains approximately 14% of the variation of bank attrition. Surprisingly, `Total_Trans_Amt` has a much lower percentage of variation of bank attrition explained (approximately 5%) even though both variables are highly correlated with each other. Therefore, we will keep `Total_Trans_Ct` and drop `Total_Trans_Amt`.\n",
    "\n",
    "The other variables not included in the first list do not explain a large percentage of variation in churn so we will drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Numerical vs. Numerical (r^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_numerical_positive**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_numerical_negative**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.2 Key Findings**\n",
    "\n",
    "`Credit_Limit`,`Avg_Open_To_Buy`,0.872472: Since Avg_Open_To_Buy is part of the equation of solving Credit_Limit, this correlation is not suprising. We will drop both features since both are not correlated highly with the target variable.\n",
    "\n",
    "`Total_Trans_Ct`,`Total_Trans_Amt`,0.771498: Since Total_Trans_Ct is more correlated with the target variable, we will keep it and drop Total_Trans_Amt.\n",
    "\n",
    "`Customer_Age`,`Months_on_book`,0.591667: We will drop Months_on_book.\n",
    "\n",
    "`Total_Revolving_Bal`,`Avg_Utilization_Ratio`,0.507283: We will just keep Total_Revolving_Bal since it is used in the equation to find Avg_Utilization_Ratio and it is more correlated with the target variable.\n",
    "\n",
    "`Avg_Open_To_Buy`,`Avg_Utilization_Ratio`,0.459306: We will  drop both features because Avg_Open_To_Buy isn't highly correlated with the target variable and Avg_Utilization_Ratio is correlated with another feature that is highly correlated with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Categorical vs. Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_variables_with_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 Categorical vs. Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_categorical_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.5 Categorical vs. Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pivot_num_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables to Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender (1), one hot encode, drop_first\n",
    "# card_category: blue or not (1), one hot encode\n",
    "# marital_status (2), one hot encode, drop_first\n",
    "# income_category (1) label\n",
    "# education_level (1) label \n",
    "\n",
    "def categorical_to_numerical(df):\n",
    "    \n",
    "    # ordinal: one hot encode\n",
    "    if 'Marital_Status' in df.columns:\n",
    "        df_dummies = pd.get_dummies(df[['Gender','Marital_Status']],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop(['Gender','Marital_Status'],axis=1,inplace=True)\n",
    "    else:\n",
    "        df_dummies = pd.get_dummies(df['Gender'],drop_first=True)\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "        df.drop('Gender',axis=1,inplace=True)\n",
    "    \n",
    "    # nominal: label encode\n",
    "    if 'Card_Category' in df.columns:\n",
    "        card_mapping = {'Blue' : 0, 'Silver' : 1, 'Gold' : 2, 'Platinum' : 3}\n",
    "        df['Card_Category'] = df['Card_Category'].map(card_mapping)\n",
    "    \n",
    "    edu_mapping = {'Uneducated' : 0, 'High School' : 1, 'College' : 2, 'Graduate' : 3, 'Post-Graduate' : 4, 'Doctorate' : 5}\n",
    "    df['Education_Level'] = df['Education_Level'].map(edu_mapping)\n",
    "    \n",
    "    # label \n",
    "    inc_mapping = {'Less than $40K' : 0, '$40K - $60K' : 1, '$60K - $80K' : 2, '$80K - $120K' : 3, '$120K +' : 4}\n",
    "    df['Income_Category'] = df['Income_Category'].map(inc_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = categorical_to_numerical(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(df):\n",
    "    df.drop(['Marital_Status_Single','Gender_M','Marital_Status_Married','Card_Category','Dependent_count','missing_marital_status','missing_education_level','missing_income_category','Contacts_Count_12_mon','Credit_Limit','Months_on_book','Avg_Open_To_Buy','Total_Trans_Amt','Avg_Utilization_Ratio','Total_Amt_Chng_Q4_Q1','Total_Relationship_Count','is_36'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = categorical_to_numerical(X_test)\n",
    "X_test_fs = X_test.copy()\n",
    "drop_features(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs= X_train.copy()\n",
    "X_train_fs = categorical_to_numerical(X_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features(X_train_fs)\n",
    "X_train_fs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "We will perform normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "scaler_fs = MinMaxScaler()\n",
    "X_train_fs = pd.DataFrame(scaler_fs.fit_transform(X_train_fs.values), columns=X_train_fs.columns, index=X_train_fs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df(scaler_type, df):\n",
    "    return pd.DataFrame(scaler_type.fit_transform(df.values), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = fit_df(scaler, X_test)\n",
    "X_test_fs = fit_df(scaler_fs, X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fs.to_csv('X_train_fs_scaled.csv',index=False)\n",
    "X_test_fs.to_csv('X_test_fs_scaled.csv',index=False)\n",
    "X_test.to_csv('X_test_scaled.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Metrics:\n",
    "\n",
    "We care the most about recall because as the credit card company, you are more concerned about people who are likely to attrite. False positives are people who we think are going to attrite but don't actually attrite. False negatives are people who we think are going to stay, but acutally levae. We care more about minimizing this group because they are more costly. WE want Recall for people who churn to be high. This means we want a large proportion of people who are positive are properly classified as positive and small portion classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_train_alg,y_pred_alg, alg):\n",
    "    # calculate p-r curves\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train_alg, y_pred_alg)\n",
    "    # convert to f score\n",
    "    fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    print(alg)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "    pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "    pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black', label=alg+' Best',zorder=10)\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    tup.sort(key = lambda x: x[1]) \n",
    "    return tup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Data Using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train_res,y_train_res)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "imba_pipeline = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              LogisticRegression())\n",
    "new_params = {'logisticregression__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True, n_iter = 100, verbose=2, n_jobs = -1)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "# grid_imba.best_params_\n",
    "# {'logisticregression__penalty': 'l2', 'logisticregression__C': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best=LogisticRegression(C=1.0,penalty=\"l2\")\n",
    "logreg_pipeline = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              logreg_best)\n",
    "logreg_pipeline.fit(X_train,y_train)\n",
    "y_pred_logreg = logreg_pipeline.predict_proba(X_train)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_logreg,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid={\"C\":np.logspace(-3,3,7), \"penalty\":['l1','l2','elasticnet']}# l1 lasso l2 ridge\n",
    "imba_pipeline_fs = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              LogisticRegression())\n",
    "new_params = {'logisticregression__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline_fs, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True, n_iter = 100, verbose=2, n_jobs = -1)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);\n",
    "# grid_imba.best_params_\n",
    "# {'logisticregression__penalty': 'l2', 'logisticregression__C': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba_fs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_best_fs=LogisticRegression(C=0.1,penalty=\"l2\")\n",
    "logreg_pipeline_fs = imblearn.pipeline.make_pipeline(SMOTE(), \n",
    "                              logreg_best_fs)\n",
    "logreg_pipeline_fs.fit(X_train_fs,y_train)\n",
    "y_pred_logreg_fs = logreg_pipeline_fs.predict_proba(X_train_fs)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_logreg_fs,'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "loss = ['deviance','exponential']\n",
    "learning_rate = [.001,.01,.1]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50,5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [20,30,50,100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [20,30,50,100]\n",
    "# Create the random grid\n",
    "random_grid = {'loss': loss,\n",
    "               'learning_rate':learning_rate,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['friedman_mse', 'mse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              GradientBoostingClassifier())\n",
    "new_params = {'gradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "\n",
    "## grid_imba.best_params_\n",
    "\n",
    "# {'gradientboostingclassifier__n_estimators': 200,\n",
    "#  'gradientboostingclassifier__min_samples_split': 30,\n",
    "#  'gradientboostingclassifier__min_samples_leaf': 50,\n",
    "#  'gradientboostingclassifier__max_features': 'auto',\n",
    "#  'gradientboostingclassifier__max_depth': 30,\n",
    "#  'gradientboostingclassifier__loss': 'deviance',\n",
    "#  'gradientboostingclassifier__learning_rate': 0.1,\n",
    "#  'gradientboostingclassifier__criterion': 'friedman_mse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_best = GradientBoostingClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=50,max_features='auto',max_depth=30,loss='deviance',learning_rate=.1,criterion='friedman_mse')\n",
    "\n",
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              gboost_best)\n",
    "imba_pipeline.fit(X_train, y_train)\n",
    "y_pred_gboost = imba_pipeline.predict_proba(X_train)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_gboost,'GBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              GradientBoostingClassifier())\n",
    "new_params = {'gradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_best_fs = GradientBoostingClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=50,max_features='auto',max_depth=30,loss='deviance',learning_rate=.1,criterion='friedman_mse')\n",
    "\n",
    "imba_pipeline_fs = make_pipeline(SMOTE(), \n",
    "                              gboost_best_fs)\n",
    "imba_pipeline_fs.fit(X_train, y_train)\n",
    "y_pred_gboost_fs = imba_pipeline_fs.predict_proba(X_train_fs)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_gboost_fs,'GBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "base_estimator = ['none',LogisticRegression()]\n",
    "# Minimum number of samples required to split a node\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# Minimum number of samples required at each leaf node\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'base_estimator': base_estimator,\n",
    "               'learning_rate': learning_rate,\n",
    "              'algorithm': algorithm}\n",
    "\n",
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              AdaBoostClassifier())\n",
    "new_params = {'adaboostclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train,y_train)\n",
    "grid_imba.best_params_\n",
    "\n",
    "# {'n_estimators': 300,\n",
    "#  'learning_rate': 0.1,\n",
    "#  'base_estimator': LogisticRegression(),\n",
    "#  'algorithm': 'SAMME.R'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best = AdaBoostClassifier(n_estimators=300,base_estimator=LogisticRegression(),learning_rate=1.0,algorithm='SAMME')\n",
    "ada_pipeline = make_pipeline(SMOTE(), \n",
    "                              ada_best)\n",
    "ada_pipeline.fit(X_train, y_train)\n",
    "y_pred_ada = ada_best.predict_proba(X_train)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_ada,'ADA Boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              AdaBoostClassifier())\n",
    "new_params = {'adaboostclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs,y_train)\n",
    "grid_imba_fs.best_params_\n",
    "\n",
    "# {'adaboostclassifier__n_estimators': 250,\n",
    "#  'adaboostclassifier__learning_rate': 0.01,\n",
    "#  'adaboostclassifier__base_estimator': LogisticRegression(),\n",
    "#  'adaboostclassifier__algorithm': 'SAMME'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best_fs = AdaBoostClassifier(n_estimators=250,base_estimator=LogisticRegression(),learning_rate=.01,algorithm='SAMME')\n",
    "ada_pipeline_fs = make_pipeline(SMOTE(), \n",
    "                              ada_best_fs)\n",
    "ada_pipeline_fs.fit(X_train_fs, y_train)\n",
    "y_pred_ada_fs = ada_best_fs.predict_proba(X_train_fs)[:,1]\n",
    "\n",
    "plot_precision_recall_curve(y_train,y_pred_ada_fs,'ADA Boost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50,5)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [20,30,50,100]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [20,30,50,100]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['entropy','gini']}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              RandomForestClassifier())\n",
    "new_params = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "##new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "grid_imba = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);\n",
    "\n",
    "# grid_imba.best_params_\n",
    "# {'randomforestclassifier__n_estimators': 200,\n",
    "#  'randomforestclassifier__min_samples_split': 30,\n",
    "#  'randomforestclassifier__min_samples_leaf': 20,\n",
    "#  'randomforestclassifier__max_features': 'log2',\n",
    "#  'randomforestclassifier__max_depth': 50,\n",
    "#  'randomforestclassifier__criterion': 'gini'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=200,min_samples_split=30,min_samples_leaf=20,max_features='log2',max_depth=50,criterion='gini')\n",
    "rf_pipeline = make_pipeline(SMOTE(), rf_best)\n",
    "rf_pipeline.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = rf_pipeline.predict_proba(X_train)[:,1]\n",
    "plot_precision_recall_curve(y_train,y_pred_rf,'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(), \n",
    "                              RandomForestClassifier())\n",
    "new_params = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n",
    "##new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "grid_imba_fs = RandomizedSearchCV(imba_pipeline, param_distributions=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba_fs.fit(X_train_fs, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba_fs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = RandomForestClassifier(n_estimators=250,min_samples_split=50,min_samples_leaf=50,max_features='log2',max_depth=20,criterion='entropy')\n",
    "rf_pipeline_fs = make_pipeline(SMOTE(), rf_best)\n",
    "rf_pipeline_fs.fit(X_train_fs,y_train)\n",
    "\n",
    "y_pred_rf_fs = rf_pipeline_fs.predict_proba(X_train_fs)[:,1]\n",
    "plot_precision_recall_curve(y_train,y_pred_rf_fs,'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Feature Selection Precision_Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'Logistic Regression'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_logreg)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "alg = 'Random Forest'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_rf)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'ADA Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_ada)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'Gradient Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_gboost)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'Logistic Regression'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_logreg_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "alg = 'Random Forest'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_rf_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "alg = 'ADA Boost'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_pred_ada_fs)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # 1 / lambda, l2 penalty\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'degree': [2,3,4],\n",
    "    # kernel coefficient\n",
    "    'gamma':[1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel':['rbf']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_fs = GridSearchCV(SVC(),param_grid,cv=3,scoring='recall')\n",
    "svc_fs.fit(X_train_res_fs,y_train_res)\n",
    "print(svc_fs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_fs = make_pipeline(SVC(C=100,degree=2,gamma=.01,kernel='rbf',probability=True))\n",
    "svc_best_fs.fit(X_train_res_fs,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc_fs = svc_best_fs.predict_proba(X_train_fs)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(y_train,y_pred_svc_fs,'SVM FS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_fs.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.class_weight_\n",
    "svm.classes_\n",
    "svm.coef_\n",
    "svm.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Without finetuning, it seems that SVM has the best recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_curves(y_train_alg,y_pred_alg,model):\n",
    "    alg = model\n",
    "    # calculate p-r curves\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train_alg, y_pred_alg)\n",
    "    # convert to f score\n",
    "    fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = argmax(fscore)\n",
    "    print(alg)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    # plot the roc curve for the model\n",
    "    no_skill = len(y_train_alg[y_train_alg==1]) / len(y_train_alg)\n",
    "    pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "    pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_curves(y_train,y_pred_logreg_fs,'Logistic Regression FS')\n",
    "plt_curves(y_train,y_pred_ada_fs,'ADA Boost FS')\n",
    "plt_curves(y_train,y_pred_gboost_fs,'Gradient Boost FS')\n",
    "plt_curves(y_train,y_pred_rf_fs,'Random Forest FS')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "plt_curves(y_train,y_pred_logreg,'Logistic Regression')\n",
    "plt_curves(y_train,y_pred_ada,'ADA Boost')\n",
    "plt_curves(y_train,y_pred_gboost,'Gradient Boost')\n",
    "plt_curves(y_train,y_pred_rf,'Random Forest')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying Key Factors\n",
    "rf_best.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_curves(y_test,rf_best.predict_proba(X_test)[:,1],'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfimp_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = categorical_to_numerical(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fs = X_test[X_train_fs.columns]\n",
    "X_test_fs = pd.DataFrame(scaler_fs.transform(X_test_fs.values), columns=X_train_fs.columns, index=X_test_fs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_test = rf_best.predict_proba(X_test)[:,1]\n",
    "y_pred_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(rf_best.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(y_pred_rf_test > 0.457846) / len(y_pred_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = to_labels(y_pred_rf_test,0.457846)\n",
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(to_labels(y_pred_rf,.457846),y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(to_labels(y_pred_rf,.457846),y_train,normalize='true').ravel()\n",
    "tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(to_labels(y_pred_rf_test,0.457846),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(to_labels(y_pred_rf_test,0.457846),y_test,normalize='true').ravel()\n",
    "tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(to_labels(y_pred_rf_test,0.295326),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(to_labels(y_pred_rf_test,0.295326),y_test,normalize='true').ravel()\n",
    "tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfimp_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gboost_test = gboost_best.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'Test'\n",
    "# calculate p-r curves\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_gboost_test)\n",
    "# convert to f score\n",
    "fscore = (5 * precision * recall) / (4 *precision+ recall)\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print(alg)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_train[y_train==1]) / len(y_train)\n",
    "pyplot.plot(recall, precision, marker='.', label=alg)\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='o', color='black',zorder=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ada_fimp_fs)\n",
    "print()\n",
    "display(randfor_fs)\n",
    "print()\n",
    "log_reg_coef_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
